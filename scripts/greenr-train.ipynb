{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![classify dandelions and grass, image from Pixabay](https://cdn.pixabay.com/photo/2018/05/20/16/13/dandelion-3416140_960_720.jpg \"image from Pixabay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Image Dataset and Train an Image Classifier using FastAI\n",
    "\n",
    "*by: Binh Phan. Inspired by [Lesson 2](https://course.fast.ai/videos/?lesson=2) of FastAI. Thanks to Francisco Ingham and Jeremy Howard* \n",
    "\n",
    "In this tutorial, we'll create an image dataset from Google Images and train a state-of-the-art image classifier extremely easily using the FastAI library. The FastAI library is built on top of the PyTorch deep learning framework, and provides commands that make training an image classifier very intuitive.\n",
    "\n",
    "For this tutorial, we'll build a dandelion vs. grass classifier. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "# import torch\n",
    "# import fastai\n",
    "# import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "# print(torch.__version__)\n",
    "# print(fastai.__version__)\n",
    "# print(torchvision.__version__)\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create an Image Dataset from Google Images**\n",
    "\n",
    "Note: this Kaggle kernel already has the dataset created from these instructions, so if you don't want to create your own dataset, feel free to skip this section and move straight to [6]\n",
    "\n",
    "**How to save a list of Google Image URLs into a csv file**\n",
    "\n",
    "Go to Google Images and search for *grass*. Initially, there will be ~50 images, so scroll down and press the button 'Show more results' at the end of the page until ~100 images have loaded. \n",
    "\n",
    "Now you must run some Javascript code in your browser which will save the URLs of all the images you want for you dataset.\n",
    "\n",
    "Press CtrlShiftJ in Windows/Linux and CmdOptJ in Mac, and a small window the javascript 'Console' will appear. That is where you will paste the JavaScript commands.\n",
    "\n",
    "Run the following commands in the prompt:\n",
    "\n",
    "```\n",
    "urls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);\n",
    "window.open('data:text/csv;charset=utf-8,' + escape(urls.join('\\n')));\n",
    "```\n",
    "\n",
    "The browser will download the file. Name the file *grass.csv*.\n",
    "\n",
    "Repeat the same steps above for *dandelion*, and save the respective file as *dandelion.csv*.\n",
    "\n",
    "**Upload the URLs as a dataset in Kaggle**\n",
    "\n",
    "In this Kaggle kernel, go to File -> Add or upload data\n",
    "\n",
    "In the top right corner, press Upload\n",
    "\n",
    "Now, add *grass.csv* and *dandelion.csv*. Name the dataset *greenr*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to do a bit of hacky work to get things to work in Kaggle. The folder /kaggle/input is read-only, and we need to manipulate that folder to download the image URLs into our folder, so we're going to move the files to another folder, /kaggle/working. That's actually the output folder, but we'll let our dataset reside there and create the outputs in the same folder. Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/greenr /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following commands to download the images from URLs into our dataset folder /kaggle/working/greenr/ using the *download_images* function.\n",
    "\n",
    "Then, we'll make sure all the images are valid using *verify_images*.\n",
    "\n",
    "After that, we'll create our dataset from *ImageDataBunch*. \n",
    "\n",
    "These are all FastAI commands that make it really easy to create a dataset :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='164' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      82.00% [164/200 00:02<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?\n",
      "Error  Invalid URL '': No schema supplied. Perhaps you meant http://?"
     ]
    }
   ],
   "source": [
    "classes = ['grass','dandelion']\n",
    "\n",
    "path = Path('/home/binhphansamsung/airflow/data')\n",
    "folder = 'grass'\n",
    "file = 'grass.csv'\n",
    "dest = path/folder\n",
    "dest.mkdir(parents=True, exist_ok=True)\n",
    "download_images(path/file, dest, max_pics=200)\n",
    "folder = 'dandelion'\n",
    "file = 'dandelion.csv'\n",
    "dest = path/folder\n",
    "dest.mkdir(parents=True, exist_ok=True)\n",
    "download_images(path/file, dest, max_pics=200)\n",
    "\n",
    "for c in classes:\n",
    "    print(c)\n",
    "    verify_images(path/c, delete=True, max_size=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view our data and see that we have a dataset. Congrats, you now have created your own image dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = ImageDataBunch.from_folder(path, train=\".\", valid_pct=0.2,\n",
    "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(rows=3, figsize=(7,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our Image Classifier\n",
    "Now, let's train an image classifer from our dataset. After this, we'll have a model that classifies dandelions vs. grass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a ResNet34 model using *cnn_learner*. ResNet34 is a pre-trained image classifier that works really well out of the box, and we're simply going to train that model on our dataset to get it to become an expert at classifying dandelions vs. grass!\n",
    "\n",
    "We'll train on the dataset, find the best learning rate, and save our model using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
    "learn.fit_one_cycle(4)\n",
    "learn.save('stage-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()\n",
    "learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))\n",
    "learn.save('stage-2')\n",
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "Let's see how well our model did using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "Some of our top losses aren't due to bad performance by our model. There are images in our data set that shouldn't be.\n",
    "\n",
    "Using the `ImageCleaner` widget from `fastai.widgets` we can prune our top losses, removing photos that don't belong.\n",
    "\n",
    "Simply mark `delete` to any image that doesn't belong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.widgets import *\n",
    "db = (ImageList.from_folder(path)\n",
    "                   .split_none()\n",
    "                   .label_from_folder()\n",
    "                   .transform(get_transforms(), size=224)\n",
    "                   .databunch()\n",
    "     )\n",
    "learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)\n",
    "\n",
    "learn_cln.load('stage-2');\n",
    "ds, idxs = DatasetFormatter().from_toplosses(learn_cln)\n",
    "ImageCleaner(ds, idxs, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also remove duplicates using this widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, idxs = DatasetFormatter().from_similars(learn_cln)\n",
    "ImageCleaner(ds, idxs, path, duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome work! Now, let's retrain our model on our pruned dataset and make it even more accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "data = ImageDataBunch.from_csv(path, folder=\".\", valid_pct=0.2, csv_labels='cleaned.csv',\n",
    "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)\n",
    "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
    "learn.fit_one_cycle(4)\n",
    "learn.save('stage-1')\n",
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot()\n",
    "learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))\n",
    "learn.save('stage-2')\n",
    "learn.load('stage-2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our confusion matrix has improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! Now, let's export our model, which will create a file named `export.pkl` in our `/kaggle/working/greenr` directory. We can now use this model to make predictions on other images, and deploy it into production! Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.device = torch.device('cpu')\n",
    "img = open_image(path/'grass'/'00000019.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path)\n",
    "pred_class,pred_idx,outputs = learn.predict(img)\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got *grass* above, then your model works! Congrats, you've now created your own image dataset and trained your own image classifier, using FastAI!\n",
    "\n",
    "If you'd like to export your model into production, simply download the `export.pkl` file and move it to wherever you want to make your predictions, like your phone or a web application :)\n",
    "\n",
    "For a live demo of a deployed web app of greenr and its source code, please visit my repository!\n",
    "\n",
    "[https://github.com/btphan95/greenr](https://github.com/btphan95/greenr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
